# ğŸ‘ï¸ EyeQcheck RAG Service

<div align="center">

![Python](https://img.shields.io/badge/python-v3.9+-blue.svg)
![FastAPI](https://img.shields.io/badge/FastAPI-005571?style=flat&logo=fastapi)
![LangChain](https://img.shields.io/badge/ğŸ¦œğŸ”—_LangChain-blue)
![OpenAI](https://img.shields.io/badge/OpenAI-412991?style=flat&logo=openai&logoColor=white)
![Pinecone](https://img.shields.io/badge/Pinecone-000000?style=flat)
![License](https://img.shields.io/badge/license-MIT-green.svg)

**AI-Powered Eye Examination Assistant with Retrieval-Augmented Generation**

_Providing intelligent, context-aware responses for eye health queries using medical knowledge retrieval and large language models._

</div>

---

## ğŸ¥ **Overview**

EyeQcheck RAG Service is the intelligent brain behind our AI-powered eye examination system. It combines:

- ğŸ“š **Medical Knowledge Retrieval** from curated eye examination documents
- ğŸ¤– **Large Language Model Integration** (OpenAI GPT-4)
- ğŸ” **Semantic Search** using Pinecone vector database
- âš¡ **Fast API Responses** with async FastAPI framework

### ğŸ¯ **Core Capabilities**

- **Medical Q&A**: Answer questions about eye examinations, symptoms, and procedures
- **Document Retrieval**: Find relevant medical information from knowledge base
- **Multi-Modal Support**: Text-based queries with future vision support
- **Context-Aware Responses**: Maintain conversation context for better assistance

---

## ğŸ—ï¸ **Architecture**

```mermaid
graph TB
    A[ğŸ‘¤ User Query] --> B[ğŸ”§ Go Backend API]
    B --> C[ğŸ RAG Service FastAPI]
    C --> D[ğŸ“Š Pinecone Vector DB]
    C --> E[ğŸ¤– OpenAI GPT-4]
    C --> F[ğŸ“š Medical Documents]

    D --> G[ğŸ” Semantic Search]
    G --> H[ğŸ“ Retrieved Context]
    H --> E
    E --> I[âœ¨ AI Response]
    I --> B
    B --> A
```

### ğŸ”„ **Request Flow**

1. **User** asks eye examination question
2. **Go Backend** forwards to RAG service
3. **RAG Service** searches medical knowledge in Pinecone
4. **Retrieved context** + user query sent to LLM
5. **AI-generated response** returned through the chain

---

## ğŸ“ **Example: Project Structure **

```
rag-service/
â”œâ”€â”€ ğŸ“„ main.py                    # FastAPI application entry point
â”œâ”€â”€ ğŸ“„ requirements.txt           # Python dependencies
â”œâ”€â”€ ğŸ“„ .env.example              # Environment variables template
â”œâ”€â”€ ğŸ“„ README.md                 # This file
â”œâ”€â”€ ğŸ“„ document.md               # Work assignment documentation
â”‚
â”œâ”€â”€ ğŸ“‚ app/
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ api/                  # API route definitions
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“‚ routes/
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ chat.py       # Main chat endpoint
â”‚   â”‚       â”œâ”€â”€ ğŸ“„ health.py     # Health check endpoint
â”‚   â”‚       â””â”€â”€ ğŸ“„ stt.py        # Speech-to-text endpoint
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ core/                 # Core application logic
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ config.py         # Configuration management
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ logging.py        # Logging configuration
â”‚   â”‚   â””â”€â”€ ğŸ“„ vectorstore.py    # Pinecone vector store manager
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ models/               # Data models and schemas
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â””â”€â”€ ğŸ“„ schemas.py        # Pydantic models
â”‚   â”‚
â”‚   â”œâ”€â”€ ğŸ“‚ services/             # Business logic services
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ llm.py           # LLM provider abstraction
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ rag.py           # RAG orchestration service
â”‚   â”‚   â””â”€â”€ ğŸ“„ retrieval.py     # Document retrieval service
â”‚   â”‚
â”‚   â””â”€â”€ ğŸ“‚ utils/                # Utility functions
â”‚       â””â”€â”€ ğŸ“„ __init__.py
â”‚
â”œâ”€â”€ ğŸ“‚ knowledge/                # Medical documents (add your docs here)
â”‚   â”œâ”€â”€ ğŸ“„ eye_examination_guide.txt
â”‚   â”œâ”€â”€ ğŸ“„ vision_tests.md
â”‚   â””â”€â”€ ğŸ“„ ...more medical docs
â”‚
â”œâ”€â”€ ğŸ“‚ tests/                    # Test files
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ test_api.py
â”‚   â”œâ”€â”€ ğŸ“„ test_rag.py
â”‚   â””â”€â”€ ğŸ“„ test_retrieval.py
â”‚
â””â”€â”€ ğŸ“‚ docs/                     # Documentation
    â”œâ”€â”€ ğŸ“„ api_reference.md
    â”œâ”€â”€ ğŸ“„ deployment.md
    â””â”€â”€ ğŸ“„ development.md
```

---

## ğŸš€ **Quick Start**

### ğŸ“‹ **Prerequisites**

- Python 3.9+ ğŸ
- OpenAI API Key ğŸ”‘
- Pinecone API Key ğŸ“Œ
- Go Backend Service (running) ğŸ”§

### âš¡ **Installation**

1. **Clone the repository**

   ```bash
   git clone <repository-url>
   cd eyeQcheck/rag-service
   ```

2. **Create virtual environment**

   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**

   ```bash
   pip install -r requirements.txt
   ```

4. **Set up environment variables**

   ```bash
   cp .env.example .env
   # Edit .env with your API keys
   ```

5. **Add medical documents**

   ```bash
   # Place your eye examination documents in knowledge/ folder
   mkdir -p knowledge
   # Add .txt, .md, or .pdf files with medical content
   ```

6. **Run the service**
   ```bash
   uvicorn main:app --reload --host 0.0.0.0 --port 8001
   ```

### ğŸ”§ **Environment Variables**

```bash
# .env file
OPENAI_API_KEY=your_openai_api_key_here
PINECONE_API_KEY=your_pinecone_api_key_here
PINECONE_ENVIRONMENT=your_pinecone_environment
PINECONE_INDEX_NAME=eyeqcheck-medical-docs

# Optional LLM Provider Settings
LLM_PROVIDER=openai  # or claude
ANTHROPIC_API_KEY=your_anthropic_key  # if using Claude

# Application Settings
LOG_LEVEL=INFO
CORS_ORIGINS=http://localhost:3000,http://localhost:8000
```

---

## ğŸ“š **API Documentation**

### ğŸ”— **Endpoints**

| Method | Endpoint        | Description               |
| ------ | --------------- | ------------------------- |
| `POST` | `/api/chat-rag` | Main RAG chat endpoint    |
| `GET`  | `/api/health`   | Health check              |
| `POST` | `/api/stt`      | Speech-to-text conversion |

### ğŸ’¬ **Chat API Example**

```bash
curl -X POST "http://localhost:8001/api/chat-rag" \
     -H "Content-Type: application/json" \
     -d '{
       "messages": [
         {"role": "user", "content": "What are the common symptoms of myopia?"}
       ],
       "temperature": 0.7
     }'
```

**Response:**

```json
{
  "reply": "Myopia, commonly known as nearsightedness, has several characteristic symptoms: 1. Difficulty seeing distant objects clearly while near vision remains good...",
  "thread_id": "thread_abc123",
  "error": null,
  "structured_data": null
}
```

### ğŸ“– **Interactive Documentation**

- **Swagger UI**: `http://localhost:8001/docs`
- **ReDoc**: `http://localhost:8001/redoc`

---

## ğŸ› ï¸ **Development**

### ğŸƒâ€â™‚ï¸ **Running in Development Mode**

```bash
# Install development dependencies
pip install -r requirements-dev.txt

# Run with auto-reload
uvicorn main:app --reload --host 0.0.0.0 --port 8001

# Run tests
pytest tests/ -v

# Code formatting
black app/ tests/
isort app/ tests/

# Linting
flake8 app/ tests/
mypy app/
```

### ğŸ§ª **Testing**

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=app tests/

# Run specific test file
pytest tests/test_rag.py -v

# Run integration tests
pytest tests/test_integration.py -v
```

### ğŸ“ **Adding New Medical Documents**

1. Add documents to `knowledge/` folder
2. Supported formats: `.txt`, `.md`, `.pdf`
3. Restart the service to reindex documents
4. Verify indexing in logs

---

## ğŸ”§ **Configuration**

### âš™ï¸ **Core Settings** (`app/core/config.py`)

```python
class Settings:
    # API Settings
    app_name: str = "EyeQcheck RAG Service"
    debug: bool = False
    cors_origins: List[str] = ["*"]

    # LLM Settings
    openai_api_key: str
    openai_model: str = "gpt-4-turbo"
    temperature: float = 0.7
    max_tokens: int = 1024

    # Vector Database
    pinecone_api_key: str
    pinecone_environment: str
    pinecone_index_name: str = "eyeqcheck-medical"

    # Document Processing
    chunk_size: int = 500
    chunk_overlap: int = 50
    embedding_model: str = "text-embedding-3-large"
```

### ğŸ›ï¸ **Customizing RAG Behavior**

**Temperature Control:**

- `0.1-0.3`: Factual, precise medical information
- `0.4-0.6`: Balanced responses
- `0.7-0.9`: More creative, conversational

**Chunk Size Optimization:**

- Small chunks (200-300): Precise information retrieval
- Medium chunks (400-600): Balanced context
- Large chunks (800-1000): Comprehensive context

---

## ğŸš¢ **Deployment**

### ğŸ³ **Docker Deployment**

```dockerfile
# Dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
EXPOSE 8001

CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8001"]
```

```bash
# Build and run
docker build -t eyeqcheck-rag .
docker run -p 8001:8001 --env-file .env eyeqcheck-rag
```
